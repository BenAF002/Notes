{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d77383",
   "metadata": {},
   "source": [
    "This chapter deals with the application of deep learning to deal with *graph-structured* data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bf5ce",
   "metadata": {},
   "source": [
    "# 13.1 Machine Learning on Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b88270",
   "metadata": {},
   "source": [
    "We can broadly group applications of ML on graph-structured data based on its goal: to predict properties of nodes, of edges, or of the graph as a whole. \n",
    "- Node prediction: Classify documents according to their topics based on their hyperlinks and citations\n",
    "- Edge prediction: Predict the presence of additional interactions (i.e. edges) within a protein network given prior knowledge about known interactions. \n",
    "- Whole graph prediction: Determine whether a molecule is water soluble. Here the target molecule is a graph, and we use a data set comprised of other graphs (molecules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373176ff",
   "metadata": {},
   "source": [
    "We can also learn useful internal representations of graphs, instead of doing plain prediction, in ***Graph Representation Learning***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaba6ac",
   "metadata": {},
   "source": [
    "GNNs define embedding vectors for each node in a graph which are then transformed through a series of learnable layers to create learned representations. We may also use learnable embeddings to represent the edges and even the graph as a whole. This is conceptually analogous to token and position embeddings in NLP DL applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d395c5b",
   "metadata": {},
   "source": [
    "## 13.1.1 Graph Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f7086",
   "metadata": {},
   "source": [
    "The text only deals with *simple graphs* possessing:\n",
    "- At most one edge between each pair of nodes\n",
    "- Only undirected edges\n",
    "- No self-edges which connect nodes back to themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d597fb",
   "metadata": {},
   "source": [
    "**Notation:**\\\n",
    "A graph $\\mathcal G = (\\mathcal V, \\mathcal E)$ consists of a set of nodes (aka. vertices) $\\mathcal V$ and edges $\\mathcal E$.\\\n",
    "We index nodes by $n=1,...,N$ and denote an edge from node $n$ to $m$ as a tuple $(n,m)$\\\n",
    "Two nodes that are linked by an edge are called *neighbors*, and the set of all neighbors for node $n$ is denoted by $\\mathcal N(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b4af6",
   "metadata": {},
   "source": [
    "The data associated with the nodes is structured into $D$-dimensional column vectors $x_n$ associated with each node $n$. The full data matrix is then $X$ with dimensionality $N\\times D$ comprised of row vectors $x_n^\\intercal$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1f184",
   "metadata": {},
   "source": [
    "## 13.1.2 Adjacency Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8f461",
   "metadata": {},
   "source": [
    "Denoted by $A$, this is used to specify the edges in a graph. Given $N$ nodes, $A$ is an $N\\times N$ symmetric matrix with each element denoting the presence of an edge *from* the node in row $i$ *to* the node in column $j$. Therefore, undirected graphs have symmetric adjacency matrices, while directed graphs have asymmetric adjacency matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3f2b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "import numpy as np\n",
    "nodes = ['a', 'b', 'c', 'd']\n",
    "edges = [('a', 'b'), ('b', 'a'), ('c', 'b'), ('c', 'd')]\n",
    "A = np.zeros([4, 4])\n",
    "for e in edges:\n",
    "    i = nodes.index(e[0])\n",
    "    j = nodes.index(e[1])\n",
    "    A[i, j] = 1\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa20ca0",
   "metadata": {},
   "source": [
    "**NOTE:**\\\n",
    "The adjacency matrix depends on the ordering of the nodes. If we changed the ordering of the nodes above such that $d$ preceded $a$, the matrix would change. Thus, the adjacency matrix is **varient** under **permutations** of the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a30ab5",
   "metadata": {},
   "source": [
    "## 13.1.3 Permutation Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672535e8",
   "metadata": {},
   "source": [
    "The *permutation matrix* $P$ specifies a particular permutation of node ordering and has the same $N\\times N$ shape as the adjacency matrix.\\\n",
    "It contains a single $1$ in each row *and* column (like Sudoku). The $1$ in position $n,m$ indicates that node $n$ will be relabeled as node $m$ after the permutation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23594e80",
   "metadata": {},
   "source": [
    "Thus, the original data matrix $X$, in which the row vectors are the data vectors at each node, can be rearranged in accord with the permutation in $P$ as:\n",
    "$$\\tilde X = PX$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6ef9b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 8 5 0]\n",
      " [6 6 9 4]\n",
      " [3 8 0 8]\n",
      " [7 3 6 9]]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[7. 3. 6. 9.]\n",
      " [6. 6. 9. 4.]\n",
      " [3. 8. 0. 8.]\n",
      " [5. 8. 5. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack([np.random.randint(0, 10, 4) for _ in nodes])\n",
    "print(X)\n",
    "P = np.eye(len(nodes), len(nodes))\n",
    "swaps = [('a', 'd')]  # swap a for d - yielding d, b, c, a\n",
    "for s in swaps:\n",
    "    i = nodes.index(s[0])\n",
    "    j = nodes.index(s[1])\n",
    "    P[i, i], P[j, j] = 0, 0  # clear diagonals\n",
    "    P[i, j], P[j, i] = 1, 1  # indicate swap\n",
    "print(P)\n",
    "X = P @ X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3284c",
   "metadata": {},
   "source": [
    "Then the adjacency matrix can be permuted by: $$\\tilde A = PAP^\\intercal$$\n",
    "This ensures that *both* the rows *and* the columns are permuted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2048df26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "A = P @ A @ P.T\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ffc2ca",
   "metadata": {},
   "source": [
    "When predicting for the entire graph want model predictions to be ***Invariant*** to the node label orderings, such that: $$y(\\tilde X, \\tilde A) = y(X, A)$$\n",
    "Where $y(\\cdot, \\cdot)$ is the network output.\n",
    "\n",
    "When predicting for individual nodes, we want the model predictions to be ***Equivariant*** to the node label orderings, such that the same node is always associated with the same prediction regardless of node order.\n",
    "$$y(\\tilde X, \\tilde A) = Py(X, A)$$\n",
    "Where $y(\\cdot, \\cdot)$ is a **vector** of network outputs, with one element per node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b17ceb",
   "metadata": {},
   "source": [
    "# 13.2 Neural Message-Passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c5e03",
   "metadata": {},
   "source": [
    "So, there are some important properties that we want GNNs to possess.\\\n",
    "If they have node-level outputs, then their entire structure must be equivariant to node ordering. That is, each layer should be equivariant under node ordering.\\\n",
    "If they have graph-level outputs, then they must also be invariant to node ordering. This may be practically achieved by introducing a final output layer that is node-order invariant rather than making all layers invariant (so long as they are still equivariant).\n",
    "\n",
    "Furthermore, because graphs can vary widely in size and complexity, we need GNNs to accept variable length inputs and to be scalable to large input sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39264e",
   "metadata": {},
   "source": [
    "## 13.2.1 Convolutional Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7222ad",
   "metadata": {},
   "source": [
    "We can apply convolutions to graphs somewhat like how we apply them to images. In an image, each pixel may be thought of as a node and each surrounding pixel may be thought to share an edge with the pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08994d44",
   "metadata": {},
   "source": [
    "A convolutional filter of order $k$, here, performs the following computation upon a *single element* (e.g. pixel) in the preceding layer $l$:\n",
    "$$z_i^{(l+1)} = f\\bigg( \\sum_j^{k^2} w_j z_j^{(l)} + b \\bigg)$$\n",
    "Where $f(\\cdot)$ is a non-linear activation function and the sum is taken over all $k^2$ elements in the convolutional patch. For instance, a $3\\times 3$ convolution would sum over up-to $9$ pixels - the primary pixel and all $8$ pixels that surround it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef062bbc",
   "metadata": {},
   "source": [
    "Note that this standard convolutional filter is not invariant under permutations of its elements. It can be made *equivariant* through modification:\n",
    "$$z_i^{(l+1)} = f\\bigg(w_{\\text{neigh}} \\sum_{j\\in\\mathcal{N}(i)}z_j^{(l)} + w_{\\text{self}}z_i^{(l)} + b \\bigg)$$\n",
    "Where $\\mathcal N(i)$ is the set of nodes neighboring node $i$ and $w_{\\text{neigh}}$ is a weight assumed to be common across the set of neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb833c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
